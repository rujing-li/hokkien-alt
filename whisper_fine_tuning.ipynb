{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrygHE31rLUd",
    "outputId": "1bedd449-503d-49e5-f8c4-6057f3766d2b"
   },
   "outputs": [],
   "source": [
    "pwd = '.'\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade datasets transformers accelerate evaluate jiwer\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# pwd = './drive/MyDrive/Colab Notebooks/CS4347'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-S-JdGhJ5e4-",
    "outputId": "b71d19f9-db39-489f-fd0f-844d8fc32c34"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import tensorboard\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import WhisperFeatureExtractor, WhisperProcessor, WhisperTokenizer, DataCollatorWithPadding, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline\n",
    "import os\n",
    "import evaluate\n",
    "from typing import Any, Dict, List, Union\n",
    "# target = 'hanlo'\n",
    "target = 'tailo'\n",
    "# target_column = 'hok_text_hanlo_tai'\n",
    "target_column = 'hok_text_tailo_number_tone'\n",
    "size = 'small' # model size\n",
    "n_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9185b93e8aff4936b1a91e68f98d2e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad1ee7af77747f08069ebaf5da2babc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd66f0241e5463097f55ccd2f584345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa49ea0fb414470a94b9ad62759f9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0a348187974b65aff1672bbaa13a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ebc37d24df4403a4d90ae5ce6005b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b80219389664800bc66e6f0439c7a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffd43fc1ed1419b8eebdc0a7d132f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8d09beca9a40d6ab07016ca1bbbd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1516 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4456af34d7884cd6b207bfb255e67d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Specify datasets and their split status\n",
    "data_sources = [\n",
    "    {\"name\": \"tat_open_source\", \"pre_split\": True},\n",
    "    {\"name\": \"hok_song\", \"pre_split\": False, \"test_split_percentage\": 1},\n",
    "    {\"name\": \"suisiann\", \"pre_split\": False, \"data_percentage\": 0.3, \"test_split_percentage\": 0.25},\n",
    "]\n",
    "\n",
    "# Initialize an empty DatasetDict\n",
    "combined_dataset = DatasetDict()\n",
    "\n",
    "# Loop through each dataset\n",
    "for data_source in data_sources:\n",
    "    dataset_name = data_source[\"name\"]\n",
    "    is_pre_split = data_source[\"pre_split\"]\n",
    "    data_percentage = data_source.get(\"data_percentage\", 1.0)  # Default to 100% if not specified\n",
    "    test_split_percentage = data_source.get(\"test_split_percentage\", 0.2) # Default to 20% if not specified\n",
    "    \n",
    "    if is_pre_split:\n",
    "        # For pre-split datasets, load train and test directly\n",
    "        dataset = load_dataset(\n",
    "            'csv',\n",
    "            data_files={\n",
    "                'train': pwd + f'/data/{dataset_name}/dev/dev.tsv',\n",
    "                'test': pwd + f'/data/{dataset_name}/test/test.tsv'\n",
    "            },\n",
    "            delimiter='\\t',\n",
    "            usecols=['hok_audio', target_column]\n",
    "        )\n",
    "    else:\n",
    "        # Load the non-pre-split dataset\n",
    "        dataset = load_dataset(\n",
    "            'csv',\n",
    "            data_files={'full': pwd + f'/data/{dataset_name}/all.csv'}\n",
    "        )\n",
    "    \n",
    "        # Filter columns using map\n",
    "        dataset = dataset['full'].map(lambda example: {key: example[key] for key in ['hok_audio', target_column]})\n",
    "    \n",
    "        # Dynamically split into train and test\n",
    "        dataset = dataset.train_test_split(test_size=test_split_percentage)\n",
    "\n",
    "    # Apply data percentage (limit the rows based on the percentage)\n",
    "    if data_percentage < 1.0:\n",
    "        dataset['train'] = dataset['train'].select(range(int(len(dataset['train']) * data_percentage)))\n",
    "        dataset['test'] = dataset['test'].select(range(int(len(dataset['test']) * data_percentage)))\n",
    "\n",
    "    def update_audio_path(example, dataset_type):\n",
    "        if is_pre_split:\n",
    "            if dataset_type == 'train':\n",
    "                example['hok_audio'] = pwd + f'/data/{dataset_name}/dev/' + example['hok_audio']\n",
    "            elif dataset_type == 'test':\n",
    "                example['hok_audio'] = pwd + f'/data/{dataset_name}/test/' + example['hok_audio']\n",
    "        else:\n",
    "            example['hok_audio'] = pwd + f'/data/{dataset_name}/' + example['hok_audio']\n",
    "        return example\n",
    "\n",
    "    dataset['train'] = dataset['train'].map(lambda x: update_audio_path(x, 'train'))\n",
    "    dataset['test'] = dataset['test'].map(lambda x: update_audio_path(x, 'test'))\n",
    "\n",
    "    # Add a `source` column to indicate the dataset name\n",
    "    dataset['train'] = dataset['train'].map(lambda x: {**x, 'source': dataset_name})\n",
    "    dataset['test'] = dataset['test'].map(lambda x: {**x, 'source': dataset_name})\n",
    "\n",
    "    # Add the current dataset's splits to the combined dataset\n",
    "    if 'train' not in combined_dataset:\n",
    "        combined_dataset['train'] = dataset['train']\n",
    "    else:\n",
    "        combined_dataset['train'] = concatenate_datasets([combined_dataset['train'], dataset['train']])\n",
    "    \n",
    "    if 'test' not in combined_dataset:\n",
    "        combined_dataset['test'] = dataset['test']\n",
    "    else:\n",
    "        combined_dataset['test'] = concatenate_datasets([combined_dataset['test'], dataset['test']])\n",
    "\n",
    "# Truncate labels for the combined dataset\n",
    "max_label_length = 448\n",
    "\n",
    "def truncate_labels(example):\n",
    "    \"\"\"Truncates the 'labels' field to the maximum allowed length.\"\"\"\n",
    "    example[target_column] = example[target_column][:max_label_length]\n",
    "    return example\n",
    "\n",
    "combined_dataset['train'] = combined_dataset['train'].map(truncate_labels)\n",
    "combined_dataset['test'] = combined_dataset['test'].map(truncate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1516\n",
      "{'hok_audio': './data/tat_open_source/dev/hok/TAT-Vol1-eval_0034_5.64_TSM013_concat.wav', 'hok_text_tailo_number_tone': 'hian7-tai7 e5 tai5-uan5 siau3-lian5-lang5 lian5“bong1 la5-a2”to1 m7 tsai1 siann2 i3-su3, beh4 an2-tsuann2 ka7 kai2-sueh4“kiam1 se2 khoo3”?', 'source': 'tat_open_source'}\n"
     ]
    }
   ],
   "source": [
    "# test dataset loading\n",
    "print(combined_dataset['train'].num_rows)\n",
    "print(combined_dataset['train'][710])\n",
    "# print(combined_dataset['train'][730])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXtl4ZjV5bQD",
    "outputId": "4f6c1f0f-5336-422e-a2c8-38dfb0875e13"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490f9917bb3a4d4dbab087fac83191af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aaddd5d852947ca8a176d1be09b6412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdde1e0d2ce4c7ca547611d62233484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dedf6c20cdb44b69efc22d4b99cf73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ddf081f510492fa15002ddf512064e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5027f0e54ec45cd9206fb43be1eb0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca737aa19896433f9e029bfde72558a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320065db6157401fac4e05f47e0bfac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-' + size)\n",
    "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-' + size, language='Mandarin', task='transcribe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fkW3QiS16tVt"
   },
   "outputs": [],
   "source": [
    "input_str = combined_dataset['train'][0][target_column]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZqUTHRP67J3",
    "outputId": "66ae160e-f82a-49ed-f42c-d4f2c26ce4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the5-si7 kha2 pian1-ho7:TA_0009\n",
      "[50258, 50260, 50359, 50363, 3322, 20, 12, 7691, 22, 350, 1641, 17, 32198, 16, 12, 1289, 22, 25, 8241, 62, 1360, 24, 50257]\n",
      "<|startoftranscript|><|zh|><|transcribe|><|notimestamps|>the5-si7 kha2 pian1-ho7:TA_0009<|endoftext|>\n",
      "the5-si7 kha2 pian1-ho7:TA_0009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "print(input_str)\n",
    "print(labels)\n",
    "print(decoded_with_special)\n",
    "print(decoded_str)\n",
    "input_str == decoded_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9uVObIiK6_Op"
   },
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained('openai/whisper-' + size, language='Mandarin', task='transcribe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "X5l3w4ikAnZH"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2c18a2b0d2472380808cf2e61051c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1516 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62acd0fa0ebe45ee8c37109a146baa39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_path = examples['hok_audio']\n",
    "    # Load audio\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "    # Resample if necessary\n",
    "    speech_array = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)(speech_array)\n",
    "    # Convert audio to log-mel spectrogram\n",
    "    input_features = processor(speech_array.squeeze().numpy(), sampling_rate=16000).input_features\n",
    "    return {'input_features': input_features, 'transcription': examples[target_column]}\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio_path = batch['hok_audio']\n",
    "    # Load audio\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    speech_array = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)(speech_array)\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] =  feature_extractor(speech_array.squeeze().numpy(), sampling_rate=16000).input_features[0]\n",
    "    # batch[\"input_features\"] = feature_extractor(speech_array, sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[target_column]).input_ids\n",
    "    return batch\n",
    "\n",
    "combined_dataset = combined_dataset.map(prepare_dataset, remove_columns=['hok_audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C20vnrMt7BQE"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6764a8559542416cbad4df6455c7f8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40066223f3140bea8902e81c2a4c0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2399656f7114409ab6dbc7e60c5862c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained Whisper model\n",
    "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-' + size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uhHz_tQh7DNC"
   },
   "outputs": [],
   "source": [
    "model.generation_config.language = 'Mandarin'\n",
    "model.generation_config.task = 'transcribe'\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ko8ssZQC7FVL"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "euytdSLI7Hpa"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SVa05cYAXMJr"
   },
   "outputs": [],
   "source": [
    "# Tailo Tokenizer\n",
    "#   code snippet from https://github.com/wchang88/Tai-Lo-Tokenizer/blob/main/TailoTokenizer.py\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "class TailoTokenizer():\n",
    "   def __init__(self):\n",
    "      self.consonants = ['ph', 'p',\n",
    "                      'm', 'b',\n",
    "                      'tshi', 'tsh', 'tsi', 'ts', 'th','t',\n",
    "                      'n', 'l',\n",
    "                      'kh', 'k',\n",
    "                      'ng', 'g',\n",
    "                      'si', 's',\n",
    "                      'ji','j',\n",
    "                      'h']\n",
    "\n",
    "   def tokenize_helper(self, word):\n",
    "      for onset in self.consonants:\n",
    "         if word.lower().find(onset) == 0:\n",
    "            if onset[-1] == 'i':\n",
    "               return [word[:len(onset)], word[len(onset) - 1:]]\n",
    "            else:\n",
    "               return [word[:len(onset)], word[len(onset):]]\n",
    "      return [word]\n",
    "\n",
    "   def tokenize(self, sent):\n",
    "      tokens = []\n",
    "      for word in re.split(r' |([%s]+)' % re.escape(punctuation), sent):\n",
    "         if word is not None:\n",
    "            if re.search(r'[%s]+' % re.escape(punctuation), word):\n",
    "               # if any combination of punctuation\n",
    "               tokens.append(word)\n",
    "            else:\n",
    "               # if a tai-lo romanization\n",
    "               tokens.extend(self.tokenize_helper(word))\n",
    "      return tokens\n",
    "\n",
    "   def tokenize_join(self, text):\n",
    "      # Tokenize into initials and finals\n",
    "      tokens = self.tokenize(text)\n",
    "      # Join tokens with spaces for consistency\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "   def tokenize_join_no_dashes(self, text): # remove \"--\"\" and \"-\"\" in Tailo (not used)\n",
    "      # Remove dashes between words\n",
    "      text = text.replace(\"--\", \" \").replace(\"-\", \" \")\n",
    "      # Tokenize into initials and finals\n",
    "      tokens = self.tokenize(text)\n",
    "      # Join tokens with spaces for consistency\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "   def remove_tone_numbers(self, token):\n",
    "      \"\"\"Removes trailing tone numbers from a token.\"\"\"\n",
    "      return re.sub(r'\\d+$', '', token)\n",
    "\n",
    "   def tokenize_join_remove_tones(self, text):\n",
    "      tokens = self.tokenize(text)\n",
    "      tokens = [self.remove_tone_numbers(token) for token in tokens]\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "   def tokenize_join_no_dashes_remove_tones(self, text):\n",
    "      text = text.replace(\"--\", \" \").replace(\"-\", \" \")\n",
    "      tokens = self.tokenize(text)\n",
    "      tokens = [self.remove_tone_numbers(token) for token in tokens]\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "   def detokenize(self, tokens):\n",
    "      i = 0\n",
    "      sentence = []\n",
    "      dash_found = False\n",
    "      while i < len(tokens):\n",
    "         if re.search(r'[%s]+' % re.escape(punctuation), tokens[i]):\n",
    "            # if the current token is punctuation\n",
    "            if '-' in tokens[i]:\n",
    "               dash_found = True\n",
    "            sentence.append(tokens[i])\n",
    "            i += 1\n",
    "         else:\n",
    "            if tokens[i] in self.consonants:\n",
    "               # if the current token is a consonant, combine it with the next\n",
    "               if tokens[i][-1] == 'i' and tokens[i+1][0] == 'i':\n",
    "                  # reduce double i into single i\n",
    "                  sentence.append(\"\".join([tokens[i], tokens[i+1][1:]]))\n",
    "               else:\n",
    "                  sentence.append(\"\".join(tokens[i:i+2]))\n",
    "               i += 2\n",
    "            else:\n",
    "               sentence.append(tokens[i])\n",
    "               i += 1\n",
    "\n",
    "            if dash_found:\n",
    "               compound = [sentence.pop() for i in range(3)]\n",
    "               sentence.append(\"\".join(compound[::-1]))\n",
    "               dash_found = False\n",
    "\n",
    "      return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91LDZAqKMhyx",
    "outputId": "b10c7b6f-a876-480a-9f49-fbed1b96ad61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sua3-loh8-lai5 khuann3 lam5-tau5-kuan7 bin5-a2-tsai3 sann1 ho7 e5 thinn1-khi3\n",
      "['s', 'ua3', '-', 'l', 'oh8', '-', 'l', 'ai5', 'kh', 'uann3', 'l', 'am5', '-', 't', 'au5', '-', 'k', 'uan7', 'b', 'in5', '-', 'a2', '-', 'ts', 'ai3', 's', 'ann1', 'h', 'o7', 'e5', 'th', 'inn1', '-', 'kh', 'i3']\n",
      "s ua3 - l oh8 - l ai5 kh uann3 l am5 - t au5 - k uan7 b in5 - a2 - ts ai3 s ann1 h o7 e5 th inn1 - kh i3\n",
      "s ua3 l oh8 l ai5 kh uann3 l am5 t au5 k uan7 b in5 a2 ts ai3 s ann1 h o7 e5 th inn1 kh i3\n",
      "s ua - l oh - l ai kh uann l am - t au - k uan b in - a - ts ai s ann h o e th inn - kh i\n",
      "s ua l oh l ai kh uann l am t au k uan b in a ts ai s ann h o e th inn kh i\n"
     ]
    }
   ],
   "source": [
    "# test Tailo Tokenizer\n",
    "text = combined_dataset['train'][2][target_column]\n",
    "tailo_tokenizer = TailoTokenizer()\n",
    "tailo_tokens_split = tailo_tokenizer.tokenize(text)\n",
    "tailo_tokens_string = tailo_tokenizer.tokenize_join(text)\n",
    "tailo_tokens_string_no_dashes = tailo_tokenizer.tokenize_join_no_dashes(text)\n",
    "\n",
    "tailo_tokens_string_no_tones = tailo_tokenizer.tokenize_join_remove_tones(text)\n",
    "tailo_tokens_string_no_dashes_no_tones = tailo_tokenizer.tokenize_join_no_dashes_remove_tones(text)\n",
    "print(text)\n",
    "print(tailo_tokens_split)\n",
    "print(tailo_tokens_string)\n",
    "print(tailo_tokens_string_no_dashes)\n",
    "print(tailo_tokens_string_no_tones)\n",
    "print(tailo_tokens_string_no_dashes_no_tones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Ro9P6ojd7oCQ"
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and references\n",
    "    pred_str_raw = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str_raw = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Hanlo case: Use CER\n",
    "    if target == 'hanlo':\n",
    "        # Load CER metric\n",
    "        cer_metric = evaluate.load('cer')\n",
    "\n",
    "        # Calculate CER\n",
    "        cer = cer_metric.compute(predictions=pred_str_raw, references=label_str_raw)\n",
    "\n",
    "        # Print examples for debugging\n",
    "        for i in range(min(5, len(pred_str_raw))):  # Print first 5 examples\n",
    "            print(f\"Prediction: {pred_str_raw[i]}\")\n",
    "            print(f\"Ground Truth: {label_str_raw[i]}\")\n",
    "            print(\"---\")\n",
    "\n",
    "        return {\n",
    "            \"cer\": 100 * cer  # CER as percentage\n",
    "        }\n",
    "\n",
    "    # Tailo case: Calculate multiple metrics\n",
    "    else:\n",
    "        # Initialize TailoTokenizer\n",
    "        tailo_tokenizer = TailoTokenizer()\n",
    "\n",
    "        # Processed strings for different metrics\n",
    "        pred_str_tokenize = [tailo_tokenizer.tokenize_join(p) for p in pred_str_raw]\n",
    "        label_str_tokenize = [tailo_tokenizer.tokenize_join(l) for l in label_str_raw]\n",
    "\n",
    "        pred_str_no_tones = [tailo_tokenizer.tokenize_join_remove_tones(p) for p in pred_str_raw]\n",
    "        label_str_no_tones = [tailo_tokenizer.tokenize_join_remove_tones(l) for l in label_str_raw]\n",
    "\n",
    "        # Load WER metric\n",
    "        wer_metric = evaluate.load('wer')\n",
    "\n",
    "        # Calculate WER for raw text\n",
    "        wer = wer_metric.compute(predictions=pred_str_raw, references=label_str_raw)\n",
    "\n",
    "        # SER for tokenized text (after `tokenize_join`)\n",
    "        ser = wer_metric.compute(predictions=pred_str_tokenize, references=label_str_tokenize)\n",
    "\n",
    "        # SER for tokenized text with tones removed (after `tokenize_join_remove_tones`)\n",
    "        ser_no_tones = wer_metric.compute(predictions=pred_str_no_tones, references=label_str_no_tones)\n",
    "\n",
    "        # Print examples for debugging\n",
    "        for i in range(min(5, len(pred_str_raw))):  # Print first 5 examples\n",
    "            print(f\"Original Prediction: {pred_str_raw[i]}\")\n",
    "            print(f\"Original Ground Truth: {label_str_raw[i]}\")\n",
    "            print(f\"Tokenized Prediction: {pred_str_tokenize[i]}\")\n",
    "            print(f\"Tokenized Ground Truth: {label_str_tokenize[i]}\")\n",
    "            print(f\"Prediction without Tones: {pred_str_no_tones[i]}\")\n",
    "            print(f\"Ground Truth without Tones: {label_str_no_tones[i]}\")\n",
    "            print(\"---\")\n",
    "\n",
    "        # Return all metrics\n",
    "        return {\n",
    "            \"wer\": 100 * wer,  # Original WER\n",
    "            \"ser\": 100 * ser,  # SER after `tokenize_join`\n",
    "            \"ser_no_tones\": 100 * ser_no_tones  # SER after `tokenize_join_remove_tones`\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nhuvd0bQ71uv"
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./logs/\"+ target + \"-whisper-\"+ size +\"-training-logs\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=20,  # originally was 500\n",
    "    # max_steps=100,  # originally was 5000\n",
    "    num_train_epochs=n_epoch,  # Use epochs instead of max_steps\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\" if target == \"hanlo\" else \"ser\", \n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7I8oartC7_o0",
    "outputId": "61b24b01-b16c-4294-f202-adaeec5be377"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4074/1919396851.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=combined_dataset['train'],\n",
    "    eval_dataset=combined_dataset['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "GozXA4HB8FnC",
    "outputId": "bae81204-9d11-4d3c-d22c-776aa68e867c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='475' max='475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [475/475 21:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=475, training_loss=0.5440326028121145, metrics={'train_runtime': 1310.59, 'train_samples_per_second': 5.784, 'train_steps_per_second': 0.362, 'total_flos': 2.1874773344256e+18, 'train_loss': 0.5440326028121145, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GiVNx8XO4PF",
    "outputId": "789bc9f0-9f8a-4769-8550-50fca900a464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/tailo-whisper-small-hokkien-finetuned-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = pwd + '/model/' + target +'-whisper-'+ size +'-hokkien-finetuned-' + str(n_epoch)\n",
    "print(save_path)\n",
    "model.save_pretrained(save_path)\n",
    "processor.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Oq2cx5kbnxPL",
    "outputId": "9e38d332-9e67-4686-dcc8-d53a6f13899f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 08:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbe74d967f543969d8916663fdc369b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction: suah4-loh8-lai5 khuann3 sin1-tik4-tshi7 bin5-a2-tsai3 it4-ho7 e5 thinn1-khi3.\n",
      "Original Ground Truth: sua3-loh8-lai5 khuann3 sin1-tik4-tshi7 bin5-a2-tsai3 it4 ho7 e5 thinn1-khi3\n",
      "Tokenized Prediction: s uah4 - l oh8 - l ai5 kh uann3 si in1 - t ik4 - tshi i7 b in5 - a2 - ts ai3 it4 - h o7 e5 th inn1 - kh i3 . \n",
      "Tokenized Ground Truth: s ua3 - l oh8 - l ai5 kh uann3 si in1 - t ik4 - tshi i7 b in5 - a2 - ts ai3 it4 h o7 e5 th inn1 - kh i3\n",
      "Prediction without Tones: s uah - l oh - l ai kh uann si in - t ik - tshi i b in - a - ts ai it - h o e th inn - kh i . \n",
      "Ground Truth without Tones: s ua - l oh - l ai kh uann si in - t ik - tshi i b in - a - ts ai it h o e th inn - kh i\n",
      "---\n",
      "Original Prediction: un1-too7 li7-tsap8-sann1 too7 tsi3 ti7-tsok8-tshit4 too7,loh8-hoo7 gi7-lut8 li7-tsap8 pha1, lai5-pin1 goo7-pah4 goo7-tsap8 kau3 ho7 tshiann2-lai5 tsap8 sann1 ho7 kui7-tai5 pang7-li2.\n",
      "Original Ground Truth: un1-too7 li7-tsap8-sann1 too7 tsi3 ji7-tsap8 tshit4 too7,loh8-hoo7 ki1-lut8 li7 tsap8% lai5-pin1 goo7-pah4 goo7-tsap8 kau2 ho7 tshiann2-lai5 tsap8-sann1 ho7 kui7-tai5 pan7-li2\n",
      "Tokenized Prediction: un1 - t oo7 l i7 - ts ap8 - s ann1 t oo7 tsi i3 t i7 - ts ok8 - tshi it4 t oo7 , l oh8 - h oo7 g i7 - l ut8 l i7 - ts ap8 ph a1 ,  l ai5 - p in1 g oo7 - p ah4 g oo7 - ts ap8 k au3 h o7 tshi iann2 - l ai5 ts ap8 s ann1 h o7 k ui7 - t ai5 p ang7 - l i2 . \n",
      "Tokenized Ground Truth: un1 - t oo7 l i7 - ts ap8 - s ann1 t oo7 tsi i3 ji i7 - ts ap8 tshi it4 t oo7 , l oh8 - h oo7 k i1 - l ut8 l i7 ts ap8 %  l ai5 - p in1 g oo7 - p ah4 g oo7 - ts ap8 k au2 h o7 tshi iann2 - l ai5 ts ap8 - s ann1 h o7 k ui7 - t ai5 p an7 - l i2\n",
      "Prediction without Tones: un - t oo l i - ts ap - s ann t oo tsi i t i - ts ok - tshi it t oo , l oh - h oo g i - l ut l i - ts ap ph a ,  l ai - p in g oo - p ah g oo - ts ap k au h o tshi iann - l ai ts ap s ann h o k ui - t ai p ang - l i . \n",
      "Ground Truth without Tones: un - t oo l i - ts ap - s ann t oo tsi i ji i - ts ap tshi it t oo , l oh - h oo k i - l ut l i ts ap %  l ai - p in g oo - p ah g oo - ts ap k au h o tshi iann - l ai ts ap - s ann h o k ui - t ai p an - l i\n",
      "---\n",
      "Original Prediction: long2-tsong2 peh4-pah4 khong3 ji7 khoo1,lau7 li2 kau2-tsap8 peh4 khoo1\n",
      "Original Ground Truth: long2-tsong2 peh4 pah4 khong3-ji7 khoo1,tsau7 li2 kau2-tsap8 peh4 khoo1\n",
      "Tokenized Prediction: l ong2 - ts ong2 p eh4 - p ah4 kh ong3 ji i7 kh oo1 , l au7 l i2 k au2 - ts ap8 p eh4 kh oo1\n",
      "Tokenized Ground Truth: l ong2 - ts ong2 p eh4 p ah4 kh ong3 - ji i7 kh oo1 , ts au7 l i2 k au2 - ts ap8 p eh4 kh oo1\n",
      "Prediction without Tones: l ong - ts ong p eh - p ah kh ong ji i kh oo , l au l i k au - ts ap p eh kh oo\n",
      "Ground Truth without Tones: l ong - ts ong p eh p ah kh ong - ji i kh oo , ts au l i k au - ts ap p eh kh oo\n",
      "---\n",
      "Original Prediction: kin1-a2-ji2 si7 sann1-gueh8 tsap8-sann1,pai3-lak8\n",
      "Original Ground Truth: kin1-a2-lit8 si7 sann1-gueh8 tsap8-sann1,pai3-lak8\n",
      "Tokenized Prediction: k in1 - a2 - ji i2 si i7 s ann1 - g ueh8 ts ap8 - s ann1 , p ai3 - l ak8\n",
      "Tokenized Ground Truth: k in1 - a2 - l it8 si i7 s ann1 - g ueh8 ts ap8 - s ann1 , p ai3 - l ak8\n",
      "Prediction without Tones: k in - a - ji i si i s ann - g ueh ts ap - s ann , p ai - l ak\n",
      "Ground Truth without Tones: k in - a - l it si i s ann - g ueh ts ap - s ann , p ai - l ak\n",
      "---\n",
      "Original Prediction: i1-ti7 it4 kiu2 su2-inn5 ni5 tshut4-si3\n",
      "Original Ground Truth: i1-ti7 it4 kiu2 su3 it4 ni5 tshut4-si3\n",
      "Tokenized Prediction: i1 - t i7 it4 k iu2 s u2 - inn5 n i5 tsh ut4 - si i3\n",
      "Tokenized Ground Truth: i1 - t i7 it4 k iu2 s u3 it4 n i5 tsh ut4 - si i3\n",
      "Prediction without Tones: i - t i it k iu s u - inn n i tsh ut - si i\n",
      "Ground Truth without Tones: i - t i it k iu s u it n i tsh ut - si i\n",
      "---\n",
      "{'eval_loss': 0.5222150087356567, 'eval_wer': 61.581211903908205, 'eval_ser': 22.79987621700112, 'eval_ser_no_tones': 17.465998568360774, 'eval_runtime': 766.2264, 'eval_samples_per_second': 1.236, 'eval_steps_per_second': 0.155, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UD1jgTot-lgW"
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "asr_model = WhisperForConditionalGeneration.from_pretrained(save_path)\n",
    "processor = WhisperProcessor.from_pretrained(save_path)\n",
    "\n",
    "asr_pipeline = pipeline(\"automatic-speech-recognition\",\n",
    "                        model=asr_model,\n",
    "                        tokenizer=processor.tokenizer,\n",
    "                        feature_extractor=processor.feature_extractor,\n",
    "                        chunk_length_s=30,\n",
    "                        batch_size=16,  # batch size for inference - set based on your device\n",
    "                        torch_dtype=torch_dtype,\n",
    "                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "o8FnrIysO2x7",
    "outputId": "5f5affb1-1ab3-4890-8149-583c034f494b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: {'text': 'i1-ki1 leh4 tiam2 tsiu2 tsit8-pue3 tsit8-pue3 tsit8-pue3 e5 ta1tshiau2 li2 ai3 ti7-liong7 gua2gua2 tsiu2-liong7 bo5-hu7 mai7 kah4 gua2 tshong1-khang1si5-kang1 tsit8-kang1 tsit8-kang1 e5 tsau2-hu3kuann2 tsit8-tsit8-tsit8 e5 lai5 u7-hau7', 'chunks': [{'timestamp': (0.0, 6.32), 'text': 'i1-ki1 leh4 tiam2 tsiu2 tsit8-pue3 tsit8-pue3 tsit8-pue3 e5 ta1'}, {'timestamp': (6.32, 10.52), 'text': 'tshiau2 li2 ai3 ti7-liong7 gua2'}, {'timestamp': (10.52, 14.28), 'text': 'gua2 tsiu2-liong7 bo5-hu7 mai7 kah4 gua2 tshong1-khang1'}, {'timestamp': (14.28, 18.32), 'text': 'si5-kang1 tsit8-kang1 tsit8-kang1 e5 tsau2-hu3'}, {'timestamp': (18.32, 21.32), 'text': 'kuann2 tsit8-tsit8-tsit8 e5 lai5 u7-hau7'}]}\n"
     ]
    }
   ],
   "source": [
    "test_file_name = '/test_hokkien.mp3'\n",
    "test_audio_path = pwd + test_file_name\n",
    "# Perform inference on a new audio file\n",
    "transcription = asr_pipeline(test_audio_path, return_timestamps=True)\n",
    "print(f\"Transcription: {transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scMUc6QAuoD7"
   },
   "source": [
    "薰一枝一枝一枝咧點\n",
    "hun tsi̍t ki tsi̍t ki leh tiám\n",
    "\n",
    "酒一杯一杯一杯咧焦\n",
    "tsiú tsi̍t pue tsi̍t pue tsi̍t pue leh ta\n",
    "\n",
    "請你愛體諒我\n",
    "tshiánn lí ài thé-liōng guá\n",
    "\n",
    "我酒量無好　莫共我創空\n",
    "guá tsiú-liōng bô hó, mài kā guá tshòng-khang\n",
    "\n",
    "時間一工一工一工咧走\n",
    "sî-kan tsi̍t kang tsi̍t kang tsi̍t kang leh tsáu\n",
    "\n",
    "汗一滴一滴一滴咧流\n",
    "kuann tsi̍t tih tsi̍t tih tsi̍t tih leh lâu\n",
    "\n",
    "有一工　咱攏老\n",
    "ū tsi̍t kang, lán lóng lāu\n",
    "\n",
    "𤆬某囝鬥陣\n",
    "tshuā bóo-kiánn tàu-tīn\n",
    "\n",
    "浪子回頭\n",
    "lōng-tsú huê-thâu"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
