{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrygHE31rLUd",
    "outputId": "1bedd449-503d-49e5-f8c4-6057f3766d2b"
   },
   "outputs": [],
   "source": [
    "pwd = '.'\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade datasets transformers accelerate evaluate jiwer\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# pwd = './drive/MyDrive/Colab Notebooks/CS4347'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-S-JdGhJ5e4-",
    "outputId": "b71d19f9-db39-489f-fd0f-844d8fc32c34"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import tensorboard\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import WhisperFeatureExtractor, WhisperProcessor, WhisperTokenizer, DataCollatorWithPadding, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline\n",
    "import os\n",
    "import evaluate\n",
    "from typing import Any, Dict, List, Union\n",
    "# target = 'hanlo'\n",
    "target = 'tailo'\n",
    "# target_column = 'hok_text_hanlo_tai'\n",
    "target_column = 'hok_text_tailo_number_tone'\n",
    "size = 'small' # model size\n",
    "n_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4443a671f6874b80b8be2f4461f836d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba247e257304fe8b4392b3acfbde967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Specify datasets and their split status\n",
    "data_sources = [\n",
    "    {\"name\": \"tat_open_source\", \"pre_split\": True},\n",
    "    # {\"name\": \"hok_song\", \"pre_split\": False, \"test_split_percentage\": 1},\n",
    "    # {\"name\": \"suisiann\", \"pre_split\": False, \"data_percentage\": 0.1, \"test_split_percentage\": 0.25},\n",
    "]\n",
    "\n",
    "# Initialize an empty DatasetDict\n",
    "combined_dataset = DatasetDict()\n",
    "\n",
    "# Loop through each dataset\n",
    "for data_source in data_sources:\n",
    "    dataset_name = data_source[\"name\"]\n",
    "    is_pre_split = data_source[\"pre_split\"]\n",
    "    data_percentage = data_source.get(\"data_percentage\", 1.0)  # Default to 100% if not specified\n",
    "    test_split_percentage = data_source.get(\"test_split_percentage\", 0.2) # Default to 20% if not specified\n",
    "    \n",
    "    if is_pre_split:\n",
    "        # For pre-split datasets, load train and test directly\n",
    "        dataset = load_dataset(\n",
    "            'csv',\n",
    "            data_files={\n",
    "                'train': pwd + f'/data/{dataset_name}/dev/dev.tsv',\n",
    "                'test': pwd + f'/data/{dataset_name}/test/test.tsv'\n",
    "            },\n",
    "            delimiter='\\t',\n",
    "            usecols=['hok_audio', target_column]\n",
    "        )\n",
    "    else:\n",
    "        # Load the non-pre-split dataset\n",
    "        dataset = load_dataset(\n",
    "            'csv',\n",
    "            data_files={'full': pwd + f'/data/{dataset_name}/all.csv'}\n",
    "        )\n",
    "    \n",
    "        # Filter columns using map\n",
    "        dataset = dataset['full'].map(lambda example: {key: example[key] for key in ['hok_audio', target_column]})\n",
    "    \n",
    "        # Dynamically split into train and test\n",
    "        dataset = dataset.train_test_split(test_size=test_split_percentage)\n",
    "\n",
    "    # Apply data percentage (limit the rows based on the percentage)\n",
    "    if data_percentage < 1.0:\n",
    "        dataset['train'] = dataset['train'].select(range(int(len(dataset['train']) * data_percentage)))\n",
    "        dataset['test'] = dataset['test'].select(range(int(len(dataset['test']) * data_percentage)))\n",
    "\n",
    "    def update_audio_path(example, dataset_type):\n",
    "        if is_pre_split:\n",
    "            if dataset_type == 'train':\n",
    "                example['hok_audio'] = pwd + f'/data/{dataset_name}/dev/' + example['hok_audio']\n",
    "            elif dataset_type == 'test':\n",
    "                example['hok_audio'] = pwd + f'/data/{dataset_name}/test/' + example['hok_audio']\n",
    "        else:\n",
    "            example['hok_audio'] = pwd + f'/data/{dataset_name}/' + example['hok_audio']\n",
    "        return example\n",
    "\n",
    "    dataset['train'] = dataset['train'].map(lambda x: update_audio_path(x, 'train'))\n",
    "    dataset['test'] = dataset['test'].map(lambda x: update_audio_path(x, 'test'))\n",
    "\n",
    "    # Add a `source` column to indicate the dataset name\n",
    "    dataset['train'] = dataset['train'].map(lambda x: {**x, 'source': dataset_name})\n",
    "    dataset['test'] = dataset['test'].map(lambda x: {**x, 'source': dataset_name})\n",
    "\n",
    "    # Add the current dataset's splits to the combined dataset\n",
    "    if 'train' not in combined_dataset:\n",
    "        combined_dataset['train'] = dataset['train']\n",
    "    else:\n",
    "        combined_dataset['train'] = concatenate_datasets([combined_dataset['train'], dataset['train']])\n",
    "    \n",
    "    if 'test' not in combined_dataset:\n",
    "        combined_dataset['test'] = dataset['test']\n",
    "    else:\n",
    "        combined_dataset['test'] = concatenate_datasets([combined_dataset['test'], dataset['test']])\n",
    "\n",
    "# Truncate labels for the combined dataset\n",
    "max_label_length = 448\n",
    "\n",
    "def truncate_labels(example):\n",
    "    \"\"\"Truncates the 'labels' field to the maximum allowed length.\"\"\"\n",
    "    example[target_column] = example[target_column][:max_label_length]\n",
    "    return example\n",
    "\n",
    "combined_dataset['train'] = combined_dataset['train'].map(truncate_labels)\n",
    "combined_dataset['test'] = combined_dataset['test'].map(truncate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722\n",
      "{'hok_audio': './data/tat_open_source/dev/hok/TAT-Vol1-eval_0034_5.64_TSM013_concat.wav', 'hok_text_tailo_number_tone': 'hian7-tai7 e5 tai5-uan5 siau3-lian5-lang5 lian5“bong1 la5-a2”to1 m7 tsai1 siann2 i3-su3, beh4 an2-tsuann2 ka7 kai2-sueh4“kiam1 se2 khoo3”?', 'source': 'tat_open_source'}\n"
     ]
    }
   ],
   "source": [
    "# test dataset loading\n",
    "print(combined_dataset['train'].num_rows)\n",
    "print(combined_dataset['train'][710])\n",
    "# print(combined_dataset['train'][730])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXtl4ZjV5bQD",
    "outputId": "4f6c1f0f-5336-422e-a2c8-38dfb0875e13"
   },
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained('openai/whisper-' + size)\n",
    "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-' + size, language='Mandarin', task='transcribe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fkW3QiS16tVt"
   },
   "outputs": [],
   "source": [
    "input_str = combined_dataset['train'][0][target_column]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZqUTHRP67J3",
    "outputId": "66ae160e-f82a-49ed-f42c-d4f2c26ce4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the5-si7 kha2 pian1-ho7:TA_0009\n",
      "[50258, 50260, 50359, 50363, 3322, 20, 12, 7691, 22, 350, 1641, 17, 32198, 16, 12, 1289, 22, 25, 8241, 62, 1360, 24, 50257]\n",
      "<|startoftranscript|><|zh|><|transcribe|><|notimestamps|>the5-si7 kha2 pian1-ho7:TA_0009<|endoftext|>\n",
      "the5-si7 kha2 pian1-ho7:TA_0009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "print(input_str)\n",
    "print(labels)\n",
    "print(decoded_with_special)\n",
    "print(decoded_str)\n",
    "input_str == decoded_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9uVObIiK6_Op"
   },
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained('openai/whisper-' + size, language='Mandarin', task='transcribe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X5l3w4ikAnZH"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288cd2a981dd46e29fa999b742fe64c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1170136921c438688be441639ec445b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_path = examples['hok_audio']\n",
    "    # Load audio\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "    # Resample if necessary\n",
    "    speech_array = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)(speech_array)\n",
    "    # Convert audio to log-mel spectrogram\n",
    "    input_features = processor(speech_array.squeeze().numpy(), sampling_rate=16000).input_features\n",
    "    return {'input_features': input_features, 'transcription': examples[target_column]}\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio_path = batch['hok_audio']\n",
    "    # Load audio\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    speech_array = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)(speech_array)\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] =  feature_extractor(speech_array.squeeze().numpy(), sampling_rate=16000).input_features[0]\n",
    "    # batch[\"input_features\"] = feature_extractor(speech_array, sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[target_column]).input_ids\n",
    "    return batch\n",
    "\n",
    "combined_dataset = combined_dataset.map(prepare_dataset, remove_columns=['hok_audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "C20vnrMt7BQE"
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained Whisper model\n",
    "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-' + size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uhHz_tQh7DNC"
   },
   "outputs": [],
   "source": [
    "model.generation_config.language = 'Mandarin'\n",
    "model.generation_config.task = 'transcribe'\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ko8ssZQC7FVL"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "euytdSLI7Hpa"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SVa05cYAXMJr"
   },
   "outputs": [],
   "source": [
    "# Tailo Tokenizer\n",
    "#   code snippet from https://github.com/wchang88/Tai-Lo-Tokenizer/blob/main/TailoTokenizer.py\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "class TailoTokenizer():\n",
    "   def __init__(self):\n",
    "      self.consonants = ['ph', 'p',\n",
    "                      'm', 'b',\n",
    "                      'tshi', 'tsh', 'tsi', 'ts', 'th','t',\n",
    "                      'n', 'l',\n",
    "                      'kh', 'k',\n",
    "                      'ng', 'g',\n",
    "                      'si', 's',\n",
    "                      'ji','j',\n",
    "                      'h']\n",
    "\n",
    "   def tokenize_helper(self, word):\n",
    "      for onset in self.consonants:\n",
    "         if word.lower().find(onset) == 0:\n",
    "            if onset[-1] == 'i':\n",
    "               return [word[:len(onset)], word[len(onset) - 1:]]\n",
    "            else:\n",
    "               return [word[:len(onset)], word[len(onset):]]\n",
    "      return [word]\n",
    "\n",
    "   def tokenize(self, sent):\n",
    "      tokens = []\n",
    "      for word in re.split(r' |([%s]+)' % re.escape(punctuation), sent):\n",
    "         if word is not None:\n",
    "            if re.search(r'[%s]+' % re.escape(punctuation), word):\n",
    "               # if any combination of punctuation\n",
    "               tokens.append(word)\n",
    "            else:\n",
    "               # if a tai-lo romanization\n",
    "               tokens.extend(self.tokenize_helper(word))\n",
    "      return tokens\n",
    "\n",
    "   def tokenize_join(self, text):\n",
    "      # Tokenize into initials and finals\n",
    "      tokens = self.tokenize(text)\n",
    "      # Join tokens with spaces for consistency\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "   def tokenize_join_no_dashes(self, text): # remove \"--\"\" and \"-\"\" in Tailo (not used)\n",
    "      # Remove dashes between words\n",
    "      text = text.replace(\"--\", \" \").replace(\"-\", \" \")\n",
    "      # Tokenize into initials and finals\n",
    "      tokens = self.tokenize(text)\n",
    "      # Join tokens with spaces for consistency\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "   def remove_tone_numbers(self, token):\n",
    "      \"\"\"Removes trailing tone numbers from a token.\"\"\"\n",
    "      return re.sub(r'\\d+$', '', token)\n",
    "\n",
    "   def tokenize_join_remove_tones(self, text):\n",
    "      tokens = self.tokenize(text)\n",
    "      tokens = [self.remove_tone_numbers(token) for token in tokens]\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "   def tokenize_join_no_dashes_remove_tones(self, text):\n",
    "      text = text.replace(\"--\", \" \").replace(\"-\", \" \")\n",
    "      tokens = self.tokenize(text)\n",
    "      tokens = [self.remove_tone_numbers(token) for token in tokens]\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "   def detokenize(self, tokens):\n",
    "      i = 0\n",
    "      sentence = []\n",
    "      dash_found = False\n",
    "      while i < len(tokens):\n",
    "         if re.search(r'[%s]+' % re.escape(punctuation), tokens[i]):\n",
    "            # if the current token is punctuation\n",
    "            if '-' in tokens[i]:\n",
    "               dash_found = True\n",
    "            sentence.append(tokens[i])\n",
    "            i += 1\n",
    "         else:\n",
    "            if tokens[i] in self.consonants:\n",
    "               # if the current token is a consonant, combine it with the next\n",
    "               if tokens[i][-1] == 'i' and tokens[i+1][0] == 'i':\n",
    "                  # reduce double i into single i\n",
    "                  sentence.append(\"\".join([tokens[i], tokens[i+1][1:]]))\n",
    "               else:\n",
    "                  sentence.append(\"\".join(tokens[i:i+2]))\n",
    "               i += 2\n",
    "            else:\n",
    "               sentence.append(tokens[i])\n",
    "               i += 1\n",
    "\n",
    "            if dash_found:\n",
    "               compound = [sentence.pop() for i in range(3)]\n",
    "               sentence.append(\"\".join(compound[::-1]))\n",
    "               dash_found = False\n",
    "\n",
    "      return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91LDZAqKMhyx",
    "outputId": "b10c7b6f-a876-480a-9f49-fbed1b96ad61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sua3-loh8-lai5 khuann3 lam5-tau5-kuan7 bin5-a2-tsai3 sann1 ho7 e5 thinn1-khi3\n",
      "['s', 'ua3', '-', 'l', 'oh8', '-', 'l', 'ai5', 'kh', 'uann3', 'l', 'am5', '-', 't', 'au5', '-', 'k', 'uan7', 'b', 'in5', '-', 'a2', '-', 'ts', 'ai3', 's', 'ann1', 'h', 'o7', 'e5', 'th', 'inn1', '-', 'kh', 'i3']\n",
      "s ua3 - l oh8 - l ai5 kh uann3 l am5 - t au5 - k uan7 b in5 - a2 - ts ai3 s ann1 h o7 e5 th inn1 - kh i3\n",
      "s ua3 l oh8 l ai5 kh uann3 l am5 t au5 k uan7 b in5 a2 ts ai3 s ann1 h o7 e5 th inn1 kh i3\n",
      "s ua - l oh - l ai kh uann l am - t au - k uan b in - a - ts ai s ann h o e th inn - kh i\n",
      "s ua l oh l ai kh uann l am t au k uan b in a ts ai s ann h o e th inn kh i\n"
     ]
    }
   ],
   "source": [
    "# test Tailo Tokenizer\n",
    "text = combined_dataset['train'][2][target_column]\n",
    "tailo_tokenizer = TailoTokenizer()\n",
    "tailo_tokens_split = tailo_tokenizer.tokenize(text)\n",
    "tailo_tokens_string = tailo_tokenizer.tokenize_join(text)\n",
    "tailo_tokens_string_no_dashes = tailo_tokenizer.tokenize_join_no_dashes(text)\n",
    "\n",
    "tailo_tokens_string_no_tones = tailo_tokenizer.tokenize_join_remove_tones(text)\n",
    "tailo_tokens_string_no_dashes_no_tones = tailo_tokenizer.tokenize_join_no_dashes_remove_tones(text)\n",
    "print(text)\n",
    "print(tailo_tokens_split)\n",
    "print(tailo_tokens_string)\n",
    "print(tailo_tokens_string_no_dashes)\n",
    "print(tailo_tokens_string_no_tones)\n",
    "print(tailo_tokens_string_no_dashes_no_tones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ro9P6ojd7oCQ"
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and references\n",
    "    pred_str_raw = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str_raw = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Hanlo case: Use CER\n",
    "    if target == 'hanlo':\n",
    "        # Load CER metric\n",
    "        cer_metric = evaluate.load('cer')\n",
    "\n",
    "        # Calculate CER\n",
    "        cer = cer_metric.compute(predictions=pred_str_raw, references=label_str_raw)\n",
    "\n",
    "        # Print examples for debugging\n",
    "        for i in range(min(5, len(pred_str_raw))):  # Print first 5 examples\n",
    "            print(f\"Prediction: {pred_str_raw[i]}\")\n",
    "            print(f\"Ground Truth: {label_str_raw[i]}\")\n",
    "            print(\"---\")\n",
    "\n",
    "        return {\n",
    "            \"cer\": 100 * cer  # CER as percentage\n",
    "        }\n",
    "\n",
    "    # Tailo case: Calculate multiple metrics\n",
    "    else:\n",
    "        # Initialize TailoTokenizer\n",
    "        tailo_tokenizer = TailoTokenizer()\n",
    "\n",
    "        # Processed strings for different metrics\n",
    "        pred_str_tokenize = [tailo_tokenizer.tokenize_join(p) for p in pred_str_raw]\n",
    "        label_str_tokenize = [tailo_tokenizer.tokenize_join(l) for l in label_str_raw]\n",
    "\n",
    "        pred_str_no_tones = [tailo_tokenizer.tokenize_join_remove_tones(p) for p in pred_str_raw]\n",
    "        label_str_no_tones = [tailo_tokenizer.tokenize_join_remove_tones(l) for l in label_str_raw]\n",
    "\n",
    "        # Load WER metric\n",
    "        wer_metric = evaluate.load('wer')\n",
    "\n",
    "        # Calculate WER for raw text\n",
    "        wer = wer_metric.compute(predictions=pred_str_raw, references=label_str_raw)\n",
    "\n",
    "        # SER for tokenized text (after `tokenize_join`)\n",
    "        ser = wer_metric.compute(predictions=pred_str_tokenize, references=label_str_tokenize)\n",
    "\n",
    "        # SER for tokenized text with tones removed (after `tokenize_join_remove_tones`)\n",
    "        ser_no_tones = wer_metric.compute(predictions=pred_str_no_tones, references=label_str_no_tones)\n",
    "\n",
    "        # Print examples for debugging\n",
    "        for i in range(min(5, len(pred_str_raw))):  # Print first 5 examples\n",
    "            print(f\"Original Prediction: {pred_str_raw[i]}\")\n",
    "            print(f\"Original Ground Truth: {label_str_raw[i]}\")\n",
    "            print(f\"Tokenized Prediction: {pred_str_tokenize[i]}\")\n",
    "            print(f\"Tokenized Ground Truth: {label_str_tokenize[i]}\")\n",
    "            print(f\"Prediction without Tones: {pred_str_no_tones[i]}\")\n",
    "            print(f\"Ground Truth without Tones: {label_str_no_tones[i]}\")\n",
    "            print(\"---\")\n",
    "\n",
    "        # Return all metrics\n",
    "        return {\n",
    "            \"wer\": 100 * wer,  # Original WER\n",
    "            \"ser\": 100 * ser,  # SER after `tokenize_join`\n",
    "            \"ser_no_tones\": 100 * ser_no_tones  # SER after `tokenize_join_remove_tones`\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nhuvd0bQ71uv"
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./logs/\"+ target + \"-whisper-\"+ size +\"-training-logs\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=20,  # originally was 500\n",
    "    # max_steps=100,  # originally was 5000\n",
    "    num_train_epochs=n_epoch,  # Use epochs instead of max_steps\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\" if target == \"hanlo\" else \"ser\", \n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7I8oartC7_o0",
    "outputId": "61b24b01-b16c-4294-f202-adaeec5be377"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9281/1919396851.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=combined_dataset['train'],\n",
    "    eval_dataset=combined_dataset['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "GozXA4HB8FnC",
    "outputId": "bae81204-9d11-4d3c-d22c-776aa68e867c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 08:04, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=225, training_loss=0.7765144877963596, metrics={'train_runtime': 487.7987, 'train_samples_per_second': 7.401, 'train_steps_per_second': 0.461, 'total_flos': 1.03198139154432e+18, 'train_loss': 0.7765144877963596, 'epoch': 4.945054945054945})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GiVNx8XO4PF",
    "outputId": "789bc9f0-9f8a-4769-8550-50fca900a464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/tailo-whisper-small-hokkien-finetuned-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = pwd + '/model/' + target +'-whisper-'+ size +'-hokkien-finetuned-' + str(n_epoch)\n",
    "print(save_path)\n",
    "model.save_pretrained(save_path)\n",
    "processor.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Oq2cx5kbnxPL",
    "outputId": "9e38d332-9e67-4686-dcc8-d53a6f13899f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/86 03:28 < 06:41, 0.14 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD1jgTot-lgW"
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "asr_model = WhisperForConditionalGeneration.from_pretrained(save_path)\n",
    "processor = WhisperProcessor.from_pretrained(save_path)\n",
    "\n",
    "asr_pipeline = pipeline(\"automatic-speech-recognition\",\n",
    "                        model=asr_model,\n",
    "                        tokenizer=processor.tokenizer,\n",
    "                        feature_extractor=processor.feature_extractor,\n",
    "                        chunk_length_s=30,\n",
    "                        batch_size=16,  # batch size for inference - set based on your device\n",
    "                        torch_dtype=torch_dtype,\n",
    "                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "o8FnrIysO2x7",
    "outputId": "5f5affb1-1ab3-4890-8149-583c034f494b"
   },
   "outputs": [],
   "source": [
    "test_file_name = '/test_hokkien.mp3'\n",
    "test_audio_path = pwd + test_file_name\n",
    "# Perform inference on a new audio file\n",
    "transcription = asr_pipeline(test_audio_path, return_timestamps=True)\n",
    "print(f\"Transcription: {transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scMUc6QAuoD7"
   },
   "source": [
    "薰一枝一枝一枝咧點\n",
    "hun tsi̍t ki tsi̍t ki leh tiám\n",
    "\n",
    "酒一杯一杯一杯咧焦\n",
    "tsiú tsi̍t pue tsi̍t pue tsi̍t pue leh ta\n",
    "\n",
    "請你愛體諒我\n",
    "tshiánn lí ài thé-liōng guá\n",
    "\n",
    "我酒量無好　莫共我創空\n",
    "guá tsiú-liōng bô hó, mài kā guá tshòng-khang\n",
    "\n",
    "時間一工一工一工咧走\n",
    "sî-kan tsi̍t kang tsi̍t kang tsi̍t kang leh tsáu\n",
    "\n",
    "汗一滴一滴一滴咧流\n",
    "kuann tsi̍t tih tsi̍t tih tsi̍t tih leh lâu\n",
    "\n",
    "有一工　咱攏老\n",
    "ū tsi̍t kang, lán lóng lāu\n",
    "\n",
    "𤆬某囝鬥陣\n",
    "tshuā bóo-kiánn tàu-tīn\n",
    "\n",
    "浪子回頭\n",
    "lōng-tsú huê-thâu"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python(4347)",
   "language": "python",
   "name": "4347"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
